Due to the inherent complexity, uncertainty quantification problems are typically viewed as approximation problems: one constructs a computationally efficient surrogate of the stochastic model under consideration and then studies this light representation instead.
In this work, we appeal to polynomial chaos (PC) expansions \cite{maitre2010, eldred2008} as a means of constructing such an approximation, in which the approximating functions are orthogonal polynomials.
Before we proceed to PC expansions, $\u(\o)$ should be further detailed as follows.

\subsubsection{Preprocessing of the uncertain parameters} \slab{parameter-preprocessing}
Let us refine our definition of $\u(\o) = \{ \u_i(\o): i = 1, \dots, \nparams \}$.
Each $\u_i(\o)$ is a characteristic of a single transistor (consider, for example, the effective channel length), and, therefore, each device in the electrical circuits at hand can potentially have a different value of this parameter as, in general, the variability due to process variation is not uniform.
Consequently, each $\u_i(\o)$ can be viewed as a random process $\u_i(\r, \o)$, $\r \in \domain$, defined on an appropriate spatial domain $\domain \subset \real^3$.
Since this work is system-level oriented, we model each processing element with one variable for each such random process.
More specifically, we let $\u_{ij}(\o) := \u_i(\r_j, \o)$ be the \rv\ representing the $i$th uncertain parameter at the $j$th processing element where $\r_j$ stands for the spatial location of the processing element (\eg, the center of mass).
Consequently, we redefine the uncertain parametrization of the problem as
\begin{equation} \elab{uncertain-parameters}
  \u(\o) = \{ \u_{ij}(\o): i = 1, \dots, \nparams, j = 1, \dots, \nprocs \}.
\end{equation}
\begin{remark}
Some authors prefer to split the variability of a process parameter at a spacial location into several parts, such as wafer-to-wafer, die-to-die, and within-die; see, \eg, \cite{juan2012}.
However, from the mathematical standpoint, it is sufficient to consider just one random variable per location which is adequately correlated with the other locations of interest.
\end{remark}

A description of $\u(\o)$ is an input to our analysis given by the user.
A proper (complete) way to describe a set of \rvs\ is to specify their joint probability distribution.
In practice, however, such exhaustive information is often unavailable, in particular, due to the high dimensionality in the presence of prominent dependencies inherent to the considered problem.
A more realistic assumption is the knowledge of the marginal distributions and correlation matrix of $\u(\o)$.
One should keep in mind though that, in general, the marginals and correlation matrix are not sufficient to recover the joint distribution.

Having specified $\u(\o)$, our foremost task is to transform $\u(\o)$ into a set of mutually independent \rvs\ as independence is a prerequisite of the forthcoming mathematical treatment.
To this end, an adequate probability transformation should be undertaken depending on the available information; see \cite{eldred2008} for an overview.
Denote such a transformation by $\u(\o) = \oTransform{\z(\o)}$, which relates the $\nparams \nprocs$ uncertain parameters $\u(\o) = \{ \u_{ij}(\o) \}$ with $\nvars$ independent \rvs
\begin{equation} \elab{independent-random-variables}
  \z(\o) = \{ \z_i(\o): i = 1, \dots, \nvars \}.
\end{equation}

As we shall discuss later on, the stochastic dimensionality $\nvars$ of the problem has a considerable impact of the computational complexity of our framework.
Therefore, an important part of the preprocessing stage is model-order reduction.
To this end, without introducing additional transformations, we let $\oTransform$ be augmented with such a reduction procedure.
Thus, $\nvars \leq \nparams \nprocs$.

Let us now give a concrete shape to the above abstract formulation by turning to our illustrative example.
We shall orientate our framework towards the practical scenario wherein the information about the distribution of $\u(\o)$ is limited.
Denote by $\{ \fCDF_{\u_i}(\hat{\u}_i): i = 1, \dots, \nparams \}$ and $\mCorrelation_\u \in \real^{\nparams \nprocs \times \nparams \nprocs}$ the marginal \cdfs\ and correlation matrix of $\{ \u_{ij}(\o) \}$ in \eref{uncertain-parameters}, respectively.
Note that $\fCDF_{\u_i}$ is the marginal of $\nprocs$ \rvs, namely, of $\{ \u_{ij}(\o): j = 1, \dots, \nprocs \}$.
Recall that we exemplify our framework considering the effective channel length and gate-oxide thickness with the notation given in \eref{application-uncertain-parameters}.
Both process parameters correspond to ordinary (Euclidean) distances; they take strictly positive values in bounded domains.
With this in mind, we model the two parameters using the four-parametric family of beta distributions:
\begin{equation*}
  \u_{ij}(\o) \sim \fCDF_{\u_i} = \betaDistribution{\alpha_i, \beta_i, a_i, b_i}
\end{equation*}
where $i \in \{ 1, 2 \}$; $j = 1, \dots, \nprocs$; $\alpha_i$ and $\beta_i$ control the shape of the distributions; and $a_i$ and $b_i$ define their supports.

The transformation $\oTransform$ that we shall utilize is the Nataf transformation \cite{li2008} since the assumed knowledge about $\u(\o)$ suffices the requirements of this technique.
Regardless of the input marginals, $\z_i \sim \GaussianDistribution{0, 1}$, $i = 1, \dots, \nvars$, in \eref{independent-random-variables}, that is, each $\z_i$ has the standard Gaussian distribution.
For clarity, all the operations performed by $\oTransform$, including model-order reduction, are discussed in the appendix, \xref{parameter-preprocessing}.

\subsubsection{Polynomial chaos expansion}
Let $\w(\o)$ be a quantity of interest dependent on $\u(\o)$.
For instance, $\w(\o)$ can correspond to the energy consumption, maximal temperature, or temperature profile of the system over a certain period of time.
In the first two cases, $\w(\o)$ is a random variable while a random matrix in the last one.
As previously mentioned, our primary tool for the probabilistic characterization of $\w(\o)$ is polynomial chaos (PC) expansions.
It can be shown that the space $\L{2}(\outcomes, \sigmaAlgebra, \probabilityMeasure)$ admits the following decomposition:
\[
  \L{2}(\outcomes, \sigmaAlgebra, \probabilityMeasure) = \bigoplus_{i = 0}^\infty \mathcal{H}_i
\]
Assume $\w(\o) \in \L{2}(\outcomes)$.
A PC expansion of $\w(\o)$ is
\begin{equation} \elab{polynomial-chaos-expansion}
  \mR(\o) = \sum_{i = 1}^\infty \pcc{\mR}_i \, \pcb_i(\vZ(\o))
\end{equation}
where $\pcc{\mR}_i$ are the coefficients of the expansion, which are matrix valued and have the same dimensionality as $\mR(\o)$.

The next step towards a PC expansion is the choice of a suitable polynomial basis $\{ \pcb_i(\vZ) \}_{i = 1}^\infty$.
Each $\pcb_i(\vZ)$ is a real-valued polynomial in terms of $\nvars$ variables (recall that $\vZ(\o) \in \real^\nvars$).
This choice depends mainly on the probability distributions of $\vZ(\o)$.
For instance, the Hermite polynomials are a natural choice for Gaussian distributions.
The primary uncertain quantity of this paper is the DSS temperature profile, denoted by the random matrix $\mQ(\o) \in \real^{\nprocs \times \nsteps}$,\footnote{As always, the argument $\o$ stands for $\vU(\o)$ or $\oTransform{\vZ(\o)}$.} of the system at hand under a certain workload, represented by the matrix $\mP_\dynamic \in \real^{\nprocs \times \nsteps}$.
For practical computations, \eref{polynomial-chaos-expansion} is truncated to preserve only $\nterms$ first polynomial terms.
The result is nothing more than a polynomial; hence, it is easy to interpret and easy to evaluate.
Consequently, such quantities as \cdfs\ and \pdfs\ can be estimated at no effort.
Moreover, the coefficients of a PC expansion immediately yield analytical formulae for the expected value and variance of the expanded quantity.

\subsubsection{Computation of the expansion coefficients}
The question now to discuss is the computation of $\{ \pcc{\mR}_i \}_{i = 1}^\nterms$ in \eref{polynomial-chaos-expansion}.
Each $\pcc{\mR}_i$ is the following multidimensional integral:
\begin{align}
  \pcc{\mR}_i & = \oExpectation{\mR(\oTransform{\vZ(\o)}) \, \pcb_i(\vZ(\o))} \nonumber \\
  & = \int \mR(\oTransform{\vZ}) \, \pcb_i(\vZ) \, \fPDF(\vZ) \, \d\vZ \elab{polynomial-chaos-coefficients}
\end{align}
where $\fPDF(\vZ)$ denotes the \pdf\ of $\vZ(\o)$.\footnote{In case of a discrete distribution, the coefficients are summations with respect to the corresponding probability mass function.}
Recall that $\pcc{\mR}_i$ is a matrix and note that the operations in \eref{polynomial-chaos-coefficients} are elementwise.

The integral in \eref{polynomial-chaos-coefficients} should be taken numerically.
In numerical integration, an integral of a function is approximated by a summation over the function values computed at a set of prescribed points and multiplied by the corresponding set of prescribed weights.
Such pairs of points and weights are called quadrature rules \cite{press2007}.
When $\mR(\o)$ is $\mQ(\o)$, for any $\vZ$ in the support of the probability distribution of $\vZ(\o)$, the needed $\mQ(\oTransform{\vZ})$ can be evaluated using \aref{deterministic-solution-with-leakage} with $\vU = \oTransform{\vZ}$.
