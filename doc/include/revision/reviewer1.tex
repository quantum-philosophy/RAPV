\begin{reviewer}
\clabel{1}{1}
The main contribution of this paper is the proposal of a methodology for
considering process variation in power and temperature modeling that is
100--200$\times$ faster than Monte Carlo simulation. The motivation for
considering process variation is well stated, and the results indeed show that
deterministic modeling when used for optimization fails to meet performance
targets once process variation is taken into account.
\end{reviewer}

\begin{authors}
Thank you.
\end{authors}

\begin{reviewer}
\clabel{1}{2}
As a proof of concept the authors propose a simple optimization algorithm for
solving the scheduling problem. They show that their methodology is able to
achieve optimization within the given constraints whereas deterministic
optimization underestimates temperature and power such that constraints are not
met. Although this optimization methodology is necessary and much faster than
na\"{i}ve methods such as Monte Carlo simulation, the reviewer feel that the
scheduling problem is a poor example to use since task scheduling is a problem
that must be solved at run time not at design time. Six hours to optimize the
scheduling for a 30ms program is not reasonable. However the reviewer can
recognize how this methodology would be quite useful for design optimization.
Perhaps a more realistic example could be used to motivate the results.
\end{reviewer}

\begin{authors}
The reviewer is right that some problems require dynamic scheduling. In this
paper, however, we consider an optimization problem (Sec.~IX-A) that involves
static scheduling, which is typically done offline, that is, at design time.
Since our work is targeted at the design stage, we consider static scheduling
to be a prominent application area of the proposed techniques.

Let us now comment on the 6 hours and 30 milliseconds mentioned by the reviewer.
We agree that such a computational demand would not be acceptable for online
usage and might not be justifiable for certain classes of offline problems.
However, as noted earlier, our methodology has been developed with design-time
applications in mind, and, in that case, computational time plays an important
but secondary role. Even days might be readily acceptable if they can help to
decrease the maintenance costs of a product which is to serve well for years.
Regarding the 30~ms, in our experiments, the execution time of \emph{one} task
is distributed between 10 and 30~ms, and the number of tasks per application
varies from 40 to 640 (Sec.~X-C). Therefore, the actual application periods,
which we apply our analysis to, are much longer than 30~ms. It is also worth
noting that the granularity of the analysis---in particular, the time step $\dt$
(Sec.~VII-C and Appendix~A)---is a variable that the user is free to adjust and
set to the value that makes the most sense to the problem under consideration.
As a result, an application with a period of one second might be comparable in
terms of optimization time with an application with a period of one minute, for
instance.

To conclude, in our opinion, a design-space-exploration problem involving
static scheduling is an important illustration of the proposed techniques, and,
therefore, the problem formulation given in Sec.~IX-A is well motivated. The
reported optimization time is affordable, especially for the industry. Lastly,
we would like to emphasize that the focal point of this work is the mathematical
foundation and basic structure of the proposed solutions, and the corresponding
implementation is merely a proof of concept, as pointed out by the reviewer.

\begin{actions}
  \action{The motivation behind static scheduling has been clarified in
  Sec.~IX-A.}
  \action{The confusion regarding the execution time of a task has been
  eliminated in Sec.~X-A.}
  \action{The number of tasks per application has been spelled out explicitly
  in Sec.~X-C.}
  \action{It has been noted in Sec.~X-A that $\dt$ might be adjusted when it is
  needed.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{1}{3}
Furthermore, the process variation is considered at the processor core level,
but in reality process variation occurs at the transistor level. The reviewer
is wondering how this methodology would scale down to finer granularity, and
whether a design optimized at the core level would actually meat the design
requirements in real silicon, where process variation occurs at the transistor
level.
\end{reviewer}

\begin{authors}
Process variation indeed takes place at the transistor level, and our work is
indeed focused on system-level analysis and optimization. From our perspective,
high-level modeling is a helpful abstraction that puts emphasize on what matters
the most, eliminates unrelated, excessive variables from the equation, and makes
the problem feasible to tackle.

Let us now discuss how the proposed solutions can be scaled down as requested by
the reviewer. We note in Sec.~V that the definition of a processing element is
flexible and can be adjusted according to the intended level of granularity
within the system level. The main scaling factor of our techniques is the
stochastic dimensionality $\nvars$, as it is explored and elaborated on in the
manuscript. Consequently, as long as $\nvars$ stays moderate, the analysis is
expected to perform well.

The dimensionality $\nvars$ does not have a direct relation with the number of
components considered, \ie, $\nprocs$, which can also be seen in Table~II. It
is primarily due to the work done by the model-order-reduction mechanism
discussed in Appendix~C; the reduction is possible and can be substantial due
to the presence of correlations induced by the fabrication process
[Friedberg,~2005]. Therefore, for a fixed problem, scaling down the granularity
level, for example, by 10 does not imply that $\nvars$ will get one level of
magnitude larger. In fact, assuming that the correlation structure stays the
same, $\nvars$ is expected to increase by little and essentially plateau as
$\nprocs$ increases.

The main question to be answered, however, is the following: To what level of
granularity does (deterministic) temperature analysis remain valid and worth
doing? First of all, one should be able to capture power profiles with a
comparable level of detail, which is doable and practical at the system level.
Second, the thermal model that we utilize was designed for architectural
studies and, hence, was verified under the corresponding conditions [Skadron,
2004]. Thus, our analysis is likely to be adequate only when it is applied at
the system level, which is what its intended use is.

To conclude, the proposed solutions have been developed for the system level
and scale well within this particular level. Low-level applications might
require certain components to be replaced, and the thermal model is a prominent
candidate in this regard.

\begin{actions}
  \action{Modeling with a finer level of granularity has been mentioned in
  Sec.~V.}
  \action{The importance of model order reduction for scaling down has been
  noted in Sec.~VI-B.}
  \action{The system-level scope of the thermal model has been mentioned in
  Sec.~VII.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{1}{4}
It is clear to the reviewer that this is an important and well developed
piece of work, and should be accepted for publication with minor revisions
addressing the two comments above.
\end{reviewer}

\begin{authors}
  Thank you for your appreciation.
\end{authors}
