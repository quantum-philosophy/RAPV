\begin{reviewer}
\clabel{1}{1}
The main contribution of this paper is the proposal of a methodology for
considering process variation in power and temperature modeling that is
100--200$\times$ faster than Monte Carlo simulation. The motivation for
considering process variation is well stated, and the results indeed show that
deterministic modeling when used for optimization fails to meet performance
targets once process variation is taken into account.
\end{reviewer}

\begin{authors}
Thank you.
\end{authors}

\begin{reviewer}
\clabel{1}{2}
As a proof of concept the authors propose a simple optimization algorithm for
solving the scheduling problem. They show that their methodology is able to
achieve optimization within the given constraints whereas deterministic
optimization underestimates temperature and power such that constraints are not
met. Although this optimization methodology is necessary and much faster than
na\"{i}ve methods such as Monte Carlo simulation, the reviewer feel that the
scheduling problem is a poor example to use since task scheduling is a problem
that must be solved at run time not at design time. Six hours to optimize the
scheduling for a 30ms program is not reasonable. However the reviewer can
recognize how this methodology would be quite useful for design optimization.
Perhaps a more realistic example could be used to motivate the results.
\end{reviewer}

\begin{authors}
The reviewer is right that some problems require dynamic scheduling. In this
paper, however, we consider an optimization problem (Sec.~IX-A) that involves
static scheduling, which is typically done offline, that is, at design time.
Since our work is targeted at the design stage, we consider static scheduling
to be a possible application area of the proposed techniques.

Let us now comment on the 6 hours and 30 milliseconds mentioned by the reviewer.
We agree that such a computational demand would not be acceptable for online
usage and might not be justifiable for certain classes of offline problems.
However, as noted earlier, our methodology has been developed with design-time
applications in mind, and, in that case, computational time plays an important
but secondary role. Even days might be readily acceptable if they can help to
decrease the maintenance costs of a product which is to serve well for years.
Regarding the 30~ms, in our experiments, the execution time of \emph{one} task
is distributed between 10 and 30~ms, and the number of tasks per application
varies from 40 to 640 (Sec.~X-C). Therefore, the actual application periods,
which we apply our analysis to, are much longer than 30~ms. It is also worth
noting that the granularity of the analysis---in particular, the time step $\dt$
(Sec.~VII-C and Appendix~A)---is a variable that the user is free to adjust and
set to the value that makes the most sense to the problem under consideration.
As a result, an application with a period of one second might be comparable in
terms of optimization time with an application with a period of one minute, for
instance.

To conclude, in our opinion, a design-space-exploration problem involving static
scheduling is an important illustration of the proposed techniques, and,
therefore, the problem formulation given in Sec.~IX-A is motivated. The reported
optimization time is affordable. Lastly, we would like to emphasize that the
focal point of this work is the mathematical foundation and basic structure of
the proposed solutions, and the corresponding implementation is merely a proof
of concept, as pointed out by the reviewer.

\begin{actions}
  \action{The motivation behind static scheduling has been clarified in
  Sec.~IX-A.}
  \action{The confusion regarding the execution time of a task has been
  eliminated in Sec.~X-A.}
  \action{The number of tasks per application has been spelled out explicitly
  in Sec.~X-C.}
  \action{It has been noted in Sec.~X-A that $\dt$ might be adjusted when it is
  needed.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{1}{3}
Furthermore, the process variation is considered at the processor core level,
but in reality process variation occurs at the transistor level. The reviewer
is wondering how this methodology would scale down to finer granularity, and
whether a design optimized at the core level would actually meat the design
requirements in real silicon, where process variation occurs at the transistor
level.
\end{reviewer}

\begin{authors}
Process variation indeed takes place at the transistor level, and our work is
indeed focused on system-level analysis and optimization. From our perspective,
high-level modeling is a helpful abstraction that puts emphasize on what matters
the most, eliminates unrelated, excessive variables from the equation, and makes
the problem feasible to tackle.

Let us now discuss how the proposed solutions can be scaled down as indicated by
the reviewer. We note in Sec.~V that the definition of a processing element is
flexible and can be adjusted according to the intended level of granularity
within the system level. A processing element can correspond to a SoC, CPU, ALU,
cache, \etc\ or even to a part of the platform that cannot be ascribed to any
particular component (imagine applying an regular grid, for example). The
thermal model that we utilize is expected to perform well at such levels of
granularity as it has been designed for architectural studies and has been
validated under the corresponding conditions [Skadron, 2004].

The main scaling factor of our techniques is the stochastic dimensionality
$\nvars$, as it is explored and elaborated on in the manuscript. Consequently,
as long as $\nvars$ stays moderate, the analysis is expected to perform well.
The dimensionality $\nvars$ does not have a direct relation with the number of
processing elements considered, that is, with $\nprocs$, which can also be seen in
Table~II. It is primarily due to the work done by the model-order-reduction
mechanism discussed in Appendix~C; the reduction is possible and can be
substantial due to the presence of correlations induced by the fabrication
process [Friedberg,~2005]. Therefore, for a fixed problem, scaling down the
granularity level, for example, by 10 does not imply that $\nvars$ will get one
level of magnitude larger. In fact, assuming that the correlation structure
stays the same, $\nvars$ is expected to increase by little and essentially
plateau as $\nprocs$ increases.

To conclude, the proposed solutions have been developed for the system level
and scale well within this particular level.

\begin{actions}
  \action{Modeling with a finer level of granularity has been mentioned in
  Sec.~V.}
  \action{The importance of model order reduction for scaling down has been
  noted in Sec.~VI-B.}
  \action{The system-level scope of the thermal model has been mentioned in
  Sec.~VII.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{1}{4}
It is clear to the reviewer that this is an important and well developed
piece of work, and should be accepted for publication with minor revisions
addressing the two comments above.
\end{reviewer}

\begin{authors}
  Thank you for your appreciation.
\end{authors}
