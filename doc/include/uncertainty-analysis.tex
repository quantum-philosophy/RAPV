The key building block of our solutions developed in \sref{temperature-analysis} and \sref{reliability-analysis} is the uncertainty quantification technique presented below.
The main task of this technique is the propagation of uncertainty through the system, that is, from a set of inputs to a set of outputs.
Specifically, the inputs are the uncertain parameters $\vu$, and the outputs are the quantities that we are interested in studying.
The latter could be, for instance, the energy consumption, maximal temperature, or temperature profile of the system over a certain period of time.

Due to the inherent complexity, uncertainty quantification problems are typically viewed as approximation problems: one constructs a computationally efficient surrogate of the stochastic model under consideration first and then studies this light representation instead.
In order to construct such an approximation, we appeal to spectral methods \cite{maitre2010, janson1997, eldred2008}.
The approach taken in this work can be classified as non-intrusive, in which case the system is treated as a ``black box.''

Before we proceed to the construction, let us first refine our definition of $\vu = (\u_i)_{i = 1}^\nparams$.
Each $\u_i$ is a characteristic of a single transistor (consider, for example, the effective channel length), and, therefore, each device in the electrical circuits at hand can potentially have a different value of this parameter as, in general, the variability due to process variation is not uniform.
Consequently, each $\u_i$ can be viewed as a random process $\u_i: \outcomes \times \domain \to \real$ defined on an appropriate spatial domain $\domain \subset \real^3$.
Since this work is system-level oriented, we model each processing element with one variable for each such random process.
More specifically, we let $\u_{ij} = \u_i(\cdot, \r_j)$ be the \rv\ representing the $i$th uncertain parameter at the $j$th processing element where $\r_j$ stands for the spatial location of the processing element (\eg, the center of mass).
Thus, we redefine the parametrization of the problem as
\begin{equation} \elab{uncertain-parameters}
  \vu = (\u_i)_{i = 1}^{\nparams \nprocs}
\end{equation}
such that there is a one-to-one correspondence between $\u_i$, $i = 1, \dots, \nparams \nprocs$, and $\u_{ij}$, $i = 1, \dots, \nparams$, $j = 1, \dots, \nprocs$.
\begin{remark}
Some authors prefer to split the variability of a process parameter at a spacial location into several parts, such as wafer-to-wafer, die-to-die, and within-die; see, \eg, \cite{juan2012}.
However, from the mathematical standpoint, it is sufficient to consider just one random variable per location which is adequately correlated with the other locations of interest.
\end{remark}

A description of $\vu$ is an input to our analysis given by the user.
A proper (complete) way to describe a set of \rvs\ is to specify their joint probability distribution.
In practice, however, such exhaustive information is often unavailable, in particular, due to the high dimensionality in the presence of prominent dependencies inherent to the considered problem.
A more realistic assumption is the knowledge of the marginal distributions and correlation matrix of $\u$.
For concreteness, we shall orientate our framework towards the practical scenario; one should keep in mind though that, in general, the marginals and correlation matrix are not sufficient to recover the joint distribution.
Denote by $\{ \distribution_{\u_i} \}_{i = 1}^{\nparams \nprocs}$ and $\mCorr_\u \in \real^{\nparams \nprocs \times \nparams \nprocs}$ the marginal distribution functions and correlation matrix of $\vu$ in \eref{uncertain-parameters}, respectively.
Note that the number of distinct marginals is only $\nparams$ since $\nprocs$ components of the random vector $\vu$ correspond to the same uncertain parameter.

\subsection{Parameter Preprocessing} \slab{parameter-preprocessing}
Our foremost task is to transform $\vu$ into mutually independent \rvs\ as independence is a prerequisite of the forthcoming mathematical treatment.
To this end, an adequate probability transformation should be undertaken depending on the available information; see \cite{eldred2008} for an overview.
One transformation for which the assumed knowledge about $\vu$ is sufficient is the Nataf transformation \cite{li2008}.
Denote this transformation by
\begin{equation} \elab{probability-transformation}
  \vu = \transformation{\vz},
\end{equation}
which relates $\nparams \nprocs$ dependent \rvs, \ie, $\vu$, with $\nvars$ independent \rvs
\begin{equation} \elab{independent-random-variables}
  \vz = (\z_i)_{i = 1}^\nvars.
\end{equation}
Regardless of the marginals, $\z_i \sim \gaussianDistribution{0, 1}$, $i = 1, \dots, \nvars$, that is, each $\z_i$ has the standard Gaussian distribution.
Refer to \xref{probability-transformation} for further details about the Nataf transformation.

As we shall discuss later on, the stochastic dimensionality $\nvars$ of the problem has a considerable impact of the computational complexity of our framework.
Therefore, an important part of the preprocessing stage is model order reduction.
To this end, without introducing additional transformations, we let $\transformation$ be augmented with such a reduction procedure.
Thus, $\nvars \leq \nparams \nprocs$.
For clarity, this duty of $\transformation$ is further discussed in \xref{model-order-reduction}.

Let us turn to the illustrative application.
Recall that we exemplify our framework considering the effective channel length and gate-oxide thickness with the notation given in \eref{application-uncertain-parameters}.
Both parameters correspond to ordinary (Euclidean) distances; they take strictly positive values in bounded domains.
With this in mind, we model the two process parameters using the four-parametric family of beta distributions:
\begin{equation*}
  \u_i \sim \distribution_{\u_i} = \betaDistribution{a_i, b_i, c_i, d_i}
\end{equation*}
where $i = 1, \dots, \nparams \nprocs$, $a_i$ and $b_i$ control the shape of the distributions, and $[ c_i, d_i ]$ correspond their supports.

\subsection{Surrogate Construction}
Let $\w \equiv \w(\vu) \equiv \w(\transformation{\vz})$ be a quantity of interest dependent on $\vu$.
In order to give a computationally efficient probabilistic characterization of $\w$, we utilize non-intrusive spectral decompositions based on orthogonal polynomials.
The corresponding mathematical foundation is outlined in \xref{spectral-decomposition} and \xref{numerical-integration}; here we leap directly to the main result.

Assuming $\w \in \L{2}(\outcomes, \sigmaAlgebra, \probabilityMeasure)$, \ie, the variance of $\w$ is finite.
Then $\w$ can be expanded into the following series:
\begin{equation} \elab{spectral-decomposition}
  \w \approx \chaos{\nvars}{\nclevel}{\w} := \sum_{\multiindex \in \chaosMultiindexSet{\nclevel}} \coefficient{\w}_{\multiindex} \, \polynomial_{\multiindex}(\vz)
\end{equation}
where $\nclevel$ is the expansion level; $\multiindex = (\alpha_i)$ is a multi-index; $\chaosMultiindexSet{\nclevel}$ is an index set to be discussed shortly; and $\polynomial_{\multiindex}(\vz)$ is an $\nvars$-variate Hermite polynomial constructed as a product of normalized one-dimensional Hermite polynomials of orders specified by the corresponding elements of $\multiindex$.

As discussed in \xref{spectral-decomposition}, each coefficient $\coefficient{\w}_{\multiindex}$ in \eref{spectral-decomposition} is an $\nvars$-dimensional integral of the product of $\w$ with $\polynomial_{\multiindex}$, and this integral should be computed numerically.
To this end, we construct a quadrature rule and calculate $\coefficient{\w}_{\multiindex}$ as
\[
  \coefficient{\w}_{\multiindex} \approx \quadrature{\nvars}{\nqlevel}{\w \, \polynomial_{\multiindex}} := \sum_{i = 1}^\nqorder \w(\transformation{\point_i}) \, \polynomial_{\multiindex}(\point_i) \, \weight_i
\]
where $\nqlevel$ is the quadrature level, and $\{ (\point_i \in \real^\nvars, \weight_i \in \real) \}_{i = 1}^\nqorder$ are the points and weights of the quadrature.
The multivariate quadrature operator $\quadrature$ is based on a set of univariate operators and is constructed as follows:
\begin{equation} \elab{smolyak-sparse-grid}
  \quadrature = \bigoplus_{\multiindex \in \quadratureMultiindexSet{\nqlevel}} \Delta_{\alpha_1} \otimes \cdots \otimes \Delta_{\alpha_\nvars}.
\end{equation}
The particular notation is not important for the present discussion and can be found in \xref{numerical-integration}.
The important aspect to note is the structure of the operator, namely, the index set $\quadratureMultiindexSet{\nqlevel}$.

The standard choice for $\multiindexSet{l}$ in \eref{spectral-decomposition} and \eref{smolyak-sparse-grid} is $\{ \multiindex: \norm[1]{\multiindex} \leq l \}$, which is referred to as isotropy since all dimensions are constrained identically.
An important generalization of the above construction is the so-called anisotropic Smolyak algorithm \cite{nobile2008}.
The main difference between the isotropic and anisotropic versions lies in the constraints imposed on $\quadratureMultiindexSet{\nqlevel}$.

\subsection{Post-processing}
The function given by \eref{spectral-decomposition} is nothing more than a polynomial; hence, it is easy to interpret and easy to evaluate.
Consequently, having constructed such an expansion, various statistics about $\w$ can be estimated at no effort.
Moreover, \eref{spectral-decomposition} immediately yields analytical formulae for the expected value and variance of $\w$.
