We consider a 45-nm technological process and rely on the 45-nm standard-cell library published and maintained by NanGate \cite{nangate}.
The effective channel length and gate-oxide thickness are assumed to have nominal values equal to 22.5~nm and 1~nm, respectively.
Following the information about process variation reported by \abbr{ITRS} \cite{itrs}, we assume that each process parameter can deviate up to 12\% of its nominal value, and this percentage is treated as three standard deviations.
The corresponding probabilistic model is the one described in \sref{parameter-preprocessing}, and the model-order reduction procedure is set to preserve 95\% of the variance of the problem (see also \xref{model-order-reduction}).
The tuning parameter $\anisotropyKnob$ in \eref{dimension-anisotropy} is set to 0.25.

Heterogeneous platforms and periodic applications are generated randomly using \abbr{TGFF} \cite{dick1998} in such a way that the execution time of tasks is uniformly distributed between 10 and 30~ms, and their dynamic power between 6 and 20~W.
The floorplans of the platforms are regular grids wherein each processing element occupies $2 \times 2\,\text{mm}^2$.
The granularity of power and temperature profiles, that is, $\dt$ in \sref{temperature-solution} and \xref{temperature-solution}, is set to 1~ms.
The stopping condition in \aref{temperature-solution} is a decrease of the normalized root-mean-square error between two successive temperature profiles smaller than 1\%, which typically requires 3--5 iterations.
In addition to the reduction of the stochastic dimensionality $\nvars$, we reduce the state-space dimensionality $\nnodes$ of the thermal system in \eref{thermal-model}.
In this case, we discard the nodes with the smallest Hankel singular values and alter the remaining ones to preserve the \abbr{DC} gain of the system.
The reduction procedure is configured to preserve 95\% of the energy of the system while ensuring that the decrease of $\nnodes$ is at most 40\%.
These constraints have been found empirically, and the corresponding reduction does not have any significant impact on accuracy.

The leakage model needed for the calculation of $\mP_\static(\vu, \mQ)$ in \aref{temperature-solution} is based on \abbr{SPICE} simulations of a series of \abbr{CMOS} invertors taken from the NanGate cell library and configured according to the high-performance 45-nm \abbr{PTM} \cite{ptm}.
The simulations are performed on a fine-grained and sufficiently broad three-dimensional grid comprising the effective channel length, gate-oxide thickness, and temperature; the results are tabulated.
The interpolation facilities of \abbr{MATLAB} \cite{matlab} are then utilized whenever we need to evaluate the leakage power for a particular point within the range of the grid.
The output of the constructed leakage model is scaled up to account for about 40\% of the total power dissipation \cite{liu2007}.

Since the optimization procedure described in \sref{reliability-optimization} embraces all the techniques developed throughout the paper, \ie, in \sref{uncertainty-analysis}--\sref[]{reliability-analysis}, we shall perform the assessment of those techniques directly in the design-space-exploration context.
In other words, we do not consider \ta\ or reliability analysis as a separate uncertainty quantification problem and shall focus on the quantification of \eref{quantity-of-interest}.

We use a genetic algorithm for optimization.
Each chromosome is a $2 \ntasks$-element vector (twice the number of tasks) concatenating a pair of $\mapping$ and $\priority$ (mapping and ranking).
The fitness function will be discussed shortly.
The population contains $4 \ntasks$ individuals which are initialized using uniform distributions.
The parents for the next generation are chosen by a tournament selection with the number of competitors equal to 20\% of $\ntasks$.
A one-point crossover is then applied to 80\% of the parents.
Each parent undergoes a uniform mutation wherein each gene is altered with probability 0.01.
The top five-percent individuals always survive.
The stopping condition is the absence of improvement within 10 successive generations.
In order to speed up the computational process, we utilize a number of auxiliary strategies and techniques described next.

The first one concerns the evaluation of a chromosome's fitness.
We begin by checking the timing constraint in \eref{timing-constraint} as it does not require any probabilistic analysis; the constraint is purely deterministic.
If \eref{timing-constraint} is violated, we set the fitness to the amount of this violation relative to the constraint---that is, to the difference between the actual application period and the deadline $\t_\maximal$ divided by $\t_\maximal$---and add a large constant, say, $C$, on top.
If \eref{timing-constraint} is satisfied, we perform our probabilistic analysis and proceed to checking \eref{thermal-constraint} and \eref{reliability-constraint}.
If any of the two is violated, we set the fitness to the total relative amount of violation plus $C/2$.
If all the constraints are satisfied, the fitness is set to the expected consumption of energy, as in shown in \eref{objective}.

Secondly, we make use of caching: the fitness value of each evaluated chromosome is stored in memory and pulled out when a chromosome with the same set of genes is encountered.

Finally, we rely on parallel computing: in each generation, unseen (not cached) individuals are assessed in parallel using 16 \abbr{CPU} cores.
This job is delegated to the parallel computing toolbox available in \abbr{MATLAB} \cite{matlab}.
