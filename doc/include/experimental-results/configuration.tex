We consider a 45-nm technological process and rely on the 45-nm standard-cell library published and maintained by NanGate \cite{nangate}.
The effective channel length and gate-oxide thickness are assumed to have nominal values equal to 22.5~nm and 1~nm, respectively.
Following the information about process variation reported by \abbr{ITRS} \cite{itrs}, we assume that each process parameter can deviate up to 12\% of its nominal value, and this percentage is treated as three standard deviations.
The corresponding probabilistic model is the one described in \sref{parameter-preprocessing} where the correlation function is taken from \cite{ukhov2014}, and the model-order reduction procedure is set to preserve 95\% of the variance of the problem (see also \xref{model-order-reduction}).
The tuning parameter $\anisotropyKnob$ in \eref{dimension-anisotropy} is set to 0.25.

Heterogeneous platforms and periodic applications are generated randomly using \abbr{TGFF} \cite{dick1998} in such a way that the execution time of tasks is uniformly distributed between 10 and 30~ms, and their dynamic power between 6 and 20~W.
The floorplans of the platforms are regular grids wherein each processing element occupies $2 \times 2\,\text{mm}^2$.
The granularity of power and temperature profiles, that is, $\dt$ in \sref{temperature-solution} and \xref{temperature-solution}, is set to 1~ms.
The stopping condition in \aref{temperature-solution} is a decrease of the normalized root-mean-square error between two successive temperature profiles smaller than 1\%, which typically requires 3--5 iterations.
In addition to the reduction of the stochastic dimensionality $\nvars$, we reduce the state-space dimensionality $\nnodes$ of the thermal system in \eref{thermal-model}.
In this case, we discard the nodes with the smallest Hankel singular values and alter the remaining ones to preserve the \abbr{DC} gain of the system.
The reduction procedure is configured to preserve 95\% of the energy of the system while ensuring that the decrease of $\nnodes$ is at most 40\%.
These constraints have been found empirically, and the corresponding reduction does not have any significant impact on accuracy.

The leakage model needed for the calculation of $\mP_\static(\vu, \mQ)$ in \aref{temperature-solution} is based on \abbr{SPICE} simulations of a series of \abbr{CMOS} invertors taken from the NanGate cell library and configured according to the high-performance 45-nm \abbr{PTM} \cite{ptm}.
The simulations are performed on a fine-grained and sufficiently broad three-dimensional grid comprising the effective channel length, gate-oxide thickness, and temperature; the results are tabulated.
The interpolation facilities of \abbr{MATLAB} \cite{matlab} are then utilized whenever we need to evaluate the leakage power for a particular point within the range of the grid.
The output of the constructed leakage model is scaled up to account for about 40\% of the total power dissipation \cite{liu2007}.

Since the optimization procedure described in \sref{reliability-optimization} embraces all the techniques developed throughout the paper, \ie, in \sref{uncertainty-analysis}--\sref[]{reliability-analysis}, we shall perform the assessment of those techniques directly in the design-space-exploration context.
In other words, we do not consider \ta\ or reliability analysis as a separate uncertainty quantification problem and shall focus on \eref{quantity-of-interest}.

We use a genetic algorithm for optimization.
Each chromosome is a $2 \ntasks$-element vector concatenating a pair of $\mapping$ and $\priority$ (mapping and ranking).
The fitness function will be discussed shortly.
The population contains $4 \ntasks$ individuals which are initialized using uniform distributions.
The parents for the next generation are chosen by a tournament selection with the number of competitors equal to 20\% of $\ntasks$.
A one-point crossover is then applied to 80\% of the parents.
Each parent undergoes a uniform mutation wherein each gene is altered with probability 0.01.
The top five-percent individuals always survive.
The stopping condition is the absence of improvement within 10 successive generations.
Since the ability to rapidly explore the design space is crucial, apart from the two reduction procedures mentioned earlier, we appeal to a number of auxiliary strategies and techniques, which we shall describe next.

The first one concerns the evaluation of a chromosome's fitness.
We begin by checking the timing constraint in \eref{timing-constraint} as it does not require any probabilistic analysis; the constraint is purely deterministic.
If \eref{timing-constraint} is violated, we set the fitness to the amount of this violation relative to the constraint---that is, to the difference between the actual application period and the deadline $\t_\maximal$ divided by $\t_\maximal$---and add a large constant, say, $C$, on top.
If \eref{timing-constraint} is satisfied, we perform our probabilistic analysis and proceed to \eref{thermal-constraint} and \eref{reliability-constraint}.
If any of the two is violated, we set the fitness to the total relative amount of violation plus $C/2$.
If all the constraints are satisfied, the fitness is set to the expected consumption of energy, as in shown in \eref{objective}.

Secondly, we make use of caching: the fitness value of each evaluated chromosome is stored in memory and pulled out when a chromosome with the same set of genes is encountered.

Finally, we rely on parallel computing: in each generation, unseen (not cached) individuals are assessed in parallel using 16 \abbr{CPU} cores.
This job is delegated to the parallel computing toolbox available in \abbr{MATLAB} \cite{matlab}.
