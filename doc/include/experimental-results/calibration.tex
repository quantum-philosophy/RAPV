Our objective here is to study the accuracy and speed of the proposed solutions.
Since the optimization procedure described in \sref{reliability-optimization} embraces all the techniques developed throughout the paper, we shall perform the assessment directly in the design-space-exploration context.
In other words, we do not consider \ta\ or reliability analysis as a separate uncertainty quantification problem in our experiments and shall focus on the quantity of interest given in \eref{quantity-of-interest}.
This quantity plays the key role as the objective function in \eref{objective} and the constraints in \eref{thermal-constraint} and \eref{reliability-constraint} are entirely based on it.

We shall compare our performance with the performance of Monte Carlo (\MC) sampling.
The operations performed by the \MC-based approach for one sample are the same as those performed by our technique for one quadrature point.
The difference is that no reduction of any kind is undertaken prior to \MC\ sampling.
In other words, the \MC-based approach samples the \emph{original} model and, hence, does not compromise any resulting accuracy.
The number of \MC\ samples is set to $10^4$, which is a practical assumption that conforms to the experience from the literature \cite{ukhov2014, lee2013, juan2012, xiang2010} and to the theoretical estimates given in \cite{diaz-emparanza2002}.
Hence, we consider this setup of \MC\ sampling to be a paragon of accuracy.

\input{include/tables/accuracy.tex}
The results concerning accuracy are displayed in \tref{accuracy} where we consider a quad-core platform, \ie, $\nprocs = 4$, with ten randomly generated applications and vary the level of polynomial expansions $\nclevel$ from one to five.
The errors for the three components of $\vw = (\qoiE, \qoiQ, \qoiT)$ are denoted by \errorE, \errorQ, and \errorT, respectively.
Each error indicator shows the distance between the empirical probability distributions produced by our approach and the ones produced by \MC\ sampling, and the measure of this distance is the popular Kullback--Leibler divergence (\abbr{KLD}) wherein the results of \MC\ sampling are treaded as the ``true'' ones.
The \abbr{KLD} takes nonnegative values and attains zero only when two distributions are equal almost everywhere \cite{durrett2010}.
In general, the errors decrease as $\nclevel$ increases.
This trend, however, is not monotonic for expansions of high levels (see \errorQ\ and \errorT\ for $\ncorder = 5$).
The observation can be ascribed to the random nature of sampling and the fact that the reduction procedures, which we undertake to gain speed, might impose limitations on the accuracy that can be attained by polynomial expansions.
\tref{accuracy} also contains the numbers of polynomial terms $\ncorder$ and quadrature points $\nqorder$ corresponding to each value of $\nclevel$.
We also performed the above experiment for platforms with fewer/more processing elements; the observations were similar to the ones in \tref{accuracy}.

Based on \tref{accuracy}, we consider the results delivered by third-level polynomial expansions, where the \abbr{KLD} drops to the third decimal place for all quantities, to be sufficiently accurate, and, therefore, we fix $\nclevel = \nqlevel = \level = 3$ (recall the notation in the last paragraph of \sref{classical-decomposition}) for the rest of the experiments.

\input{include/tables/speed.tex}
\tref{speed} displays the time needed to perform one characterization of $\vw$ for the number of processing elements $\nprocs$ swept from 2 to 32.
It can be seen that the computational time ranges from a fraction of a second to around two seconds.
More importantly, \tref{speed} provides information about a number of complementary quantities that are of high interest for the user of the proposed techniques, which we discuss below.

The primary quantity to pay attention to is the number of \rvs\ $\nvars$ preserved after the reduction procedure described in \sref{preprocessing} and \xref{model-order-reduction}.
Without this reduction, $\nvars$ would be $2 \nprocs$ as there are two process parameters per processing element.
It can be seen that there is no reduction for the dual-core platform while around 80\% of the stochastic dimensions have been eliminated for the platform with 32 cores.
In addition, one can note that $\nvars$ is the same for the last two platforms.
The magnitude of reduction is solely determined by the correlation patterns assumed (see \sref{preprocessing}) and the floorplans of the considered platforms.

Another important quantity displayed in \tref{speed} is the number of quadrature nodes $\nqorder$.
This number is the main indicator of the computational complexity of our probabilistic analysis: it equals to the number of times Algorithm~X in \aref{surrogate-construction} is executed to construct a polynomial expansion of \eref{quantity-of-interest} needed for the evaluation of the fitness function.
It can be seen that $\nqorder$ is very low.
To illustrate this, the last column of \tref{speed} shows the speedup of our approach with respect to $10^4$ \MC.
Our solution is faster by approximately 100--200 times while delivering highly accurate results as discussed earlier.
It should be noted that the comparison has been drawn based on the number of evaluation points rather than on the actual time since the relative cost of other computations is negligible.

To conclude, the proposed solutions to temperature and reliability analyses under process variation have been assessed using the compositional quantity of interest given in \eref{quantity-of-interest}.
The results shown in \tref{accuracy} and \tref{speed} allow us to conclude that our approach is both accurate and computationally efficient.
