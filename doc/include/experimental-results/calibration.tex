Our objective here is to study the accuracy and speed of the proposed solutions.
Since the optimization procedure described in \sref{reliability-optimization} embraces all the techniques developed throughout the paper, \ie, in \sref{uncertainty-analysis}--\sref[]{reliability-analysis}, we shall perform the assessment directly in the design-space-exploration context.
In other words, we do not consider \ta\ or reliability analysis as a separate uncertainty quantification problem and shall focus on the quantity of interest given in \eref{quantity-of-interest}.
This quantity plays the key role as the objective function in \eref{objective} and the constraints in \eref{thermal-constraint} and \eref{reliability-constraint} are entirely based on it.
We shall compare our performance with the performance of Monte Carlo (\MC) sampling.
The operations performed by the \MC-based approach for one sample are the same as those performed by our technique for one quadrature point.
The only difference is that no reduction of any kind is undertaken inside \MC\ sampling so that the corresponding results are accurate.
In what follows, the number of \MC\ samples is set to $10^4$ \cite{ukhov2014, lee2013, juan2012, xiang2010}.

\input{include/tables/accuracy.tex}
The results concerning accuracy are displayed in \tref{accuracy} where we consider a quad-core platform, \ie, $\nprocs = 4$, with ten randomly generated applications and vary the level of polynomial expansions $\nclevel$ from one to five.
The errors for the three components of $\vw = (\qoiE, \qoiQ, \qoiT)$ are denoted by \errorE, \errorQ, and \errorT, respectively.
Each error indicator shows the Kullback--Leibler divergence (\abbr{KLD}) of our empirical probability distribution from the one of \MC\ sampling.
In general, the errors decrease as $\nclevel$ increases.
This trend, however, is not monotonic for expansions of high orders (see \errorQ\ and \errorT\ for $\ncorder = 5$).
The observation can be ascribed to the random nature of sampling and the fact that the reduction procedures, which we undertake to gain speed, might impose limitations on the accuracy that can be attained by polynomial expansions.
\tref{accuracy} also contains the numbers of polynomial terms $\ncorder$ and quadrature points $\nqorder$ corresponding to each value of $\nclevel$.
We also performed the above experiment for platforms with fewer/more processing elements; the observations were similar to the ones in \tref{accuracy}.

Based on \tref{accuracy}, we consider the results delivered by third-level polynomial expansions, where the \abbr{KLD} is below 0.005 for the three error metrics, to be sufficiently accurate, and, therefore, we fix $\nclevel = \nqlevel = \level = 3$ (recall the notation in the last paragraph of \sref{classical-decomposition}) for the rest of the experiments.

\input{include/tables/speed.tex}
\tref{speed} displays the time needed to perform one characterization of $\vw$ for the number of processing elements $\nprocs$ swept from 2 to 32.
It can be seen that the computational time ranges from a fraction of a second to around two seconds.
More importantly, \tref{speed} provides information about a number of complementary quantities that are of high interest for the user of the proposed techniques, which we discuss below.

The first quantity to pay attention to is the number of \rvs\ $\nvars$ preserved after the reduction procedure described in \sref{parameter-preprocessing} and \xref{model-order-reduction}; without this reduction, $\nvars$ would be $2 \nprocs$ as there are two process parameters per core.
It can be seen that the dual-core platform has no reduction while around 80\% of stochastic dimensions have been eliminated for the platform with 32 cores.
In addition, one can note that $\nvars$ is the same for the last two platforms.
The magnitude of reduction is solely determined by the correlation patterns assumed (see \sref{parameter-preprocessing}) and the floorplans of the platforms.

The next quantity to look at in \tref{speed} is the number of quadrature nodes $\nqorder$.
This number is the main indicator of the computational complexity of our probabilistic analysis: it equals to the number of times Algorithm~X in \aref{surrogate-construction} is executed to construct a polynomial expansion of \eref{quantity-of-interest} needed for the evaluation of the fitness function.
It can be seen that $\nqorder$ is rather low taking into account the fact that the straightforward MC sampling typically requires thousands of samples \cite{ukhov2014, lee2013, xiang2010, juan2012}.
To illustrate this, the last column of \tref{speed} shows the speedup of our approach with respect to $10^4$ \MC\ samples.
Our solution is faster by approximately 100--200 times while delivering highly accurate results as discussed earlier.
It should be noted that the comparison has been drawn based on the number of evaluation points rather than on the actual time since the relative cost of other computations is negligible.

For the interested reader, we also show the number of thermal nodes $\nnodes$ left after the reduction of the thermal system, which was mentioned in the third paragraph of \sref{experimental-results-configuration}.
Without this reduction, $\nnodes$ would be $4 \nprocs + 12$.
In all the cases, around 40\% of nodes have been dropped, which is the maximal amount allowed by our setup.
This means that certain configurations could be reduced even further while satisfying the 95-percent constraint on the energy of the corresponding systems.
