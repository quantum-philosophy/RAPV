Electronic systems are naturally exposed to a wide range of failure mechanisms.
The most dominant examples include electromigration, time-dependent dielectric breakdown, stress migration, and thermal cycling \cite{xiang2010}; see \cite{jedec2011} for an exhaustive overview.
All of the aforementioned mechanisms have exponential dependencies on the operating temperature.

Let $\T: \outcomes \to \real$ be a \rv\ representing the lifetime of the considered system.
The lifetime is the time until the system experiences a fault after which the system no longer meets the requirements imposed on this system.
$\T$ is a function of the lifetimes of the processing elements, which are denoted by a set of \rvs\ $\{ \T_i \}_{i = 1}^\nprocs$.
Let $\distribution_\T(\cdot | \vwr)$ be the distribution of $\T$ where $\vwr = (\wr_i)$ is a vector of parameters.
The survival function of the system is defined as
\[
  \survival_\T(\t | \vwr) = 1 - \distribution_\T(\t | \vwr).
\]
Similarly, we shall denote by $\{ \distribution_{\T_i}(\cdot | \vwr) \}_{i = 1}^\nprocs$ and $\{ \survival_{\T_i}(\cdot | \vwr) \}_{i = 1}^\nprocs$ the marginal distributions and individual survival functions, respectively, of the processing elements.

In the context of reliability analysis, our primary objectives are (a) to expand the spectrum of probability distributions that accompany the state-of-the-art physical models of wear \cite{jedec2011}, (b) to enrich the corresponding reliability models with temperature analysis considering process variation, and (c) to speed up the associated computational process by eliminating the high costs of MC sampling.
To elaborate, our work is motivated by the following two observations.

First, the \apriori\ knowledge of the probability distribution of the lifetime of a system-level component or even of an entire system is a considerably strong assumption, which often fails in practice.
For example, the log-normal and Weibull families are omnipresent in the contemporary literature; however, as shown in \cite{xiang2010}, a member of either family alone can be highly inaccurate for certain classes of systems,  and one has to reside to various mixture models in order to alleviate this concern.
The need of taking such foreseeing decisions makes the reliability analysis of electronic systems stiff since the designer has to obtain ad hoc formulae for each system individually and adjust these formulae whenever an additional uncertain parameter is to be included in the analysis.

Second, assuming that an \apriori-chosen probabilistic model $\survival_\T(\cdot, \vwr)$ is sensible, the major portion of the associated computational time is ascribed to the evaluation of the parameters collected in $\vwr$ rather than to the model \perse, that is, when $\vwr$ is known.
For instance, $\vwr$ often contains estimates of the mean time to failure of each processing element given for a range of stress levels.
Therefore, $\vwr$ typically involves (computationally intensive) full-system simulations including power analysis paired with temperature analysis.

Guided by the above observations, we propose to the use of the spectral decompositions developed in \sref{uncertainty-analysis} and \sref{temperature-analysis} in order to construct a light surrogate for $\T$.
This approach allows one to seamlessly incorporate into reliability analysis the effect of process variation on process parameters.
The framework makes no assumptions about the family of distributions that $\T$ belongs to, which means that each system will attain a custom-tailored survival function $\survival_\T(\cdot | \vwr)$.
In contrast to the straightforward use of MC sampling, the spectral representation that we construct makes the subsequent analysis highly efficient from the computational perspective.

The structure of $\T$ with respect to $\{ \T_i \}_{i = 1}^\nprocs$ is problem specific, and it can be especially diverse in the context of fault-tolerant systems.
Each $\T_i$ is characterized by a physical model of wear \cite{jedec2011} capturing the fatigues that the corresponding processing element is suffering from.
Let $\vwr$ contain all the parameters that $\T$ depends on.
Then the only assumption we make is that the user is able to evaluate $\T$ for a given $\vwr$.
Our approach is very flexible in this regard as it considers $\T$ as a ``black-box'' function of $\vwr$.
Hence, the proposed solution is readily applicable to any model that the user considers to be the most adequate for the problem at hand.

Despite its generality, the proposed technique can be better appreciated considering a concrete example.
To this end, we assume that any fault of any processing element makes the whole system fail, in which case
\[
  \T = \min_{i = 1}^\nprocs \T_i.
\]

Suppose that the system is experiencing a periodic workload (\eg, due to the execution of a periodic application) with period $\period$.
Assume further that the power consumption is rapidly changing during $\period$, and, thus, the major concern of the designer is the thermal-cycling fatigue \cite{jedec2011}.
This fatigue has the most prominent dependency on temperature: apart from average/maximal temperatures, the frequencies and amplitudes of temperature fluctuations matter in this case.
Therefore, the knowledge of a temperature profile $\mQ$ is required.

Since the assumed workload is period, $\mQ$ is a \dss\ temperature profile over a time interval of length $\period$.
Using a peak-detection algorithm, for the $i$th processing element, the interval $[0, \period]$ is split into $\nsegments[\,i]$ segments $0 = \t_{i \, 0} < \t_{i \, 1} < \dotsc < \t_{i \, \nsegments[\,i]} = \period$ according to the extrama detected in $\mQ(i, :)$.
Note that this partitioning should not be confused with the sampling interval of power/temperature profiles described in \sref{problem-formulation}.
The fatigue is modeled using the family of Weibull distributions \cite{ukhov2012, xiang2010}.
